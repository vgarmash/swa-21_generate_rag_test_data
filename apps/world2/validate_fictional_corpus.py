# apps/world2/validate_fictional_corpus.py
import json
import os
import re
from collections import Counter

class FictionalCorpusValidator:
    def __init__(self, knowledge_base_folder="knowledge_base", generated_folder="generated"):
        self.knowledge_base_folder = knowledge_base_folder
        self.generated_folder = generated_folder
        self.documents = []
        self.load_documents()

        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ°Ğ¿Ğ¿Ğ¸Ğ½Ğ³ Ğ¸Ğ· generated Ğ¿Ğ°Ğ¿ĞºĞ¸
        terms_map_path = os.path.join(generated_folder, "terms_map.json")
        if os.path.exists(terms_map_path):
            with open(terms_map_path, 'r', encoding='utf-8') as f:
                self.terms_map = json.load(f)
        else:
            self.terms_map = {"term_mappings": {}}

    def load_documents(self):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· knowledge_base Ğ¿Ğ°Ğ¿ĞºĞ¸"""
        if not os.path.exists(self.knowledge_base_folder):
            print(f"âš ï¸  Directory '{self.knowledge_base_folder}' not found")
            return

        for filename in os.listdir(self.knowledge_base_folder):
            if filename.endswith('.txt'):
                with open(os.path.join(self.knowledge_base_folder, filename), 'r', encoding='utf-8') as f:
                    content = f.read()
                    self.documents.append({
                        'id': filename.replace('.txt', ''),
                        'content': content,
                        'type': filename.split('_')[0]
                    })

    def check_for_original_terms(self):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚, Ğ½Ğµ Ğ¾ÑÑ‚Ğ°Ğ»Ğ¸ÑÑŒ Ğ»Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹ ĞÑÑ‚ĞµÑ€Ğ¸ĞºÑĞ°"""
        original_terms = [
            'Asterix', 'Obelix', 'Getafix', 'Vitalstatistix', 'Cacofonix',
            'Gaul', 'Rome', 'Roman', 'mistletoe', 'boar', 'druid',
            'Babaorum', 'Laudanum', 'Aquarium', 'Petibonum'
        ]

        found_terms = {}

        for doc in self.documents:
            content_lower = doc['content'].lower()
            for term in original_terms:
                if term.lower() in content_lower:
                    found_terms.setdefault(term, []).append(doc['id'])

        if found_terms:
            print("âš ï¸  Found original Asterix terms:")
            for term, docs in found_terms.items():
                print(f"  '{term}' in documents: {', '.join(docs[:3])}{'...' if len(docs) > 3 else ''}")
            return False
        else:
            print("âœ“ No original Asterix terms found")
            return True

    def analyze_term_usage(self):
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ²"""
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹ Ğ¸Ğ· terms_map.json
        all_fictional_terms = []

        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ²Ñ‹Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹ Ğ¸Ğ· mapping
        if 'categories' in self.terms_map:
            for category, terms_dict in self.terms_map['categories'].items():
                if isinstance(terms_dict, dict):
                    all_fictional_terms.extend(terms_dict.values())

        # Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ term_mappings
        if 'term_mappings' in self.terms_map:
            term_mappings = self.terms_map['term_mappings']
            if isinstance(term_mappings, dict):
                for value in term_mappings.values():
                    if isinstance(value, str):
                        all_fictional_terms.append(value)

        term_usage = Counter()

        for doc in self.documents:
            content_lower = doc['content'].lower()
            for term in all_fictional_terms:
                if isinstance(term, str) and term.lower() in content_lower:
                    term_usage[term] += 1

        print(f"\nFound {len(term_usage)} unique fictional terms used")
        if term_usage:
            print("Top 10 most used fictional terms:")
            print("-" * 40)
            for term, count in term_usage.most_common(10):
                print(f"  {term:25}: {count:3} occurrences")
        else:
            print("âš ï¸  No fictional terms detected in documents")
            # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ±Ñ‹Ñ‚ÑŒ
            print("Example terms that should be present:")
            if 'categories' in self.terms_map:
                for category in ['characters', 'items']:
                    if category in self.terms_map['categories']:
                        terms = list(self.terms_map['categories'][category].values())[:3]
                        print(f"  {category}: {', '.join(terms)}")

        return term_usage

    def check_cross_references(self):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‘ÑÑ‚Ğ½Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸"""
        ref_pattern = r'\(see document: ([A-Z]+_\d+)\)|\(refer to: ([A-Z]+_\d+)\)|\(detailed in: ([A-Z]+_\d+)\)|\(cf\. ([A-Z]+_\d+)\)'

        all_doc_ids = {doc['id'] for doc in self.documents}
        broken_refs = []

        for doc in self.documents:
            matches = re.findall(ref_pattern, doc['content'])
            for match in matches:
                ref_id = next((m for m in match if m), None)
                if ref_id and ref_id not in all_doc_ids:
                    broken_refs.append((doc['id'], ref_id))

        if broken_refs:
            print(f"\nâš ï¸  Found {len(broken_refs)} broken references:")
            for source, target in broken_refs[:5]:
                print(f"  {source} -> {target} (missing)")
        else:
            print("\nâœ“ All cross-references are valid")

        return broken_refs

    def check_document_uniqueness(self):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²"""
        content_hashes = set()
        duplicate_count = 0

        for doc in self.documents:
            # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ…ÑÑˆ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
            content_hash = hash(doc['content'][:1000])  # ĞŸĞµÑ€Ğ²Ñ‹Ğµ 1000 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²
            if content_hash in content_hashes:
                duplicate_count += 1
                print(f"  âš ï¸  Possible duplicate: {doc['id']}")
            content_hashes.add(content_hash)

        if duplicate_count == 0:
            print("âœ“ All documents appear to be unique")
            return True
        else:
            print(f"âš ï¸  Found {duplicate_count} possible duplicate documents")
            return False

    def analyze_document_statistics(self):
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²"""
        print("\nDocument Statistics:")
        print("-" * 40)

        # Ğ¢Ğ¸Ğ¿Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
        doc_types = Counter(doc['type'] for doc in self.documents)
        for doc_type, count in doc_types.items():
            percentage = (count / len(self.documents)) * 100
            print(f"  {doc_type:15}: {count:2} documents ({percentage:.1f}%)")

        # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ»Ğ¾Ğ²
        total_words = sum(len(doc['content'].split()) for doc in self.documents)
        avg_words = total_words / len(self.documents) if self.documents else 0

        print(f"\n  Total documents: {len(self.documents)}")
        print(f"  Total words: {total_words:,}")
        print(f"  Average words per document: {avg_words:.0f}")

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑÑ‹Ğ»ĞºĞ¸
        total_refs = sum(len(re.findall(r'\(see document:|\(refer to:|\(detailed in:|\(cf\.', doc['content'])) for doc in self.documents)
        avg_refs = total_refs / len(self.documents) if self.documents else 0

        print(f"  Total cross-references: {total_refs}")
        print(f"  Average references per document: {avg_refs:.2f}")

        return {
            'total_documents': len(self.documents),
            'total_words': total_words,
            'avg_words': avg_words,
            'total_refs': total_refs,
            'avg_refs': avg_refs
        }

    def generate_coherence_report(self):
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ Ğ¾ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°"""
        print("=" * 60)
        print("FICTION CORPUS VALIDATION REPORT")
        print("=" * 60)

        if not self.documents:
            print("âš ï¸  No documents found. Please generate documents first.")
            return {"error": "No documents found"}

        # 1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹
        original_free = self.check_for_original_terms()

        # 2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
        uniqueness = self.check_document_uniqueness()

        # 3. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ²
        term_usage = self.analyze_term_usage()

        # 4. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑÑ‹Ğ»Ğ¾Ğº
        broken_refs = self.check_cross_references()

        # 5. Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
        stats = self.analyze_document_statistics()

        # 6. ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°
        print("\n" + "=" * 60)
        print("QUALITY ASSESSMENT")
        print("-" * 60)

        score = 0
        if original_free:
            score += 2
            print("âœ“ Original terms: PASS (2/2)")
        else:
            print("âœ— Original terms: FAIL (0/2)")

        if uniqueness:
            score += 2
            print("âœ“ Document uniqueness: PASS (2/2)")
        else:
            print(f"âœ— Document uniqueness: PARTIAL (1/2)")
            score += 1

        if not broken_refs:
            score += 2
            print("âœ“ Cross-references: PASS (2/2)")
        else:
            deduction = min(2, len(broken_refs) / 10)
            score += (2 - deduction)
            print(f"âœ— Cross-references: PARTIAL ({2 - deduction:.1f}/2)")

        if len(term_usage) >= 20:
            score += 2
            print("âœ“ Term diversity: PASS (2/2)")
        else:
            partial_score = min(2, len(term_usage) / 10)
            score += partial_score
            print(f"âœ— Term diversity: PARTIAL ({partial_score:.1f}/2)")

        if stats['avg_refs'] >= 1.0:
            score += 2
            print("âœ“ Interconnectivity: PASS (2/2)")
        else:
            partial_score = min(2, stats['avg_refs'])
            score += partial_score
            print(f"âœ— Interconnectivity: PARTIAL ({partial_score:.1f}/2)")

        print(f"\n  FINAL SCORE: {score}/10 ({score/10*100:.1f}%)")

        report = {
            'original_free': original_free,
            'uniqueness': uniqueness,
            'broken_refs': len(broken_refs),
            'unique_terms': len(term_usage),
            'total_documents': stats['total_documents'],
            'avg_references': stats['avg_refs'],
            'quality_score': score,
            'quality_percentage': score/10*100
        }

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ Ğ² generated Ğ¿Ğ°Ğ¿ĞºĞµ
        report_path = os.path.join(self.generated_folder, "validation_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nâœ“ Validation report saved to {report_path}")

        return report

def main():
    """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸"""
    print("ğŸ” Validating Fictional Corpus")
    print("=" * 60)

    validator = FictionalCorpusValidator("knowledge_base", "generated")
    report = validator.generate_coherence_report()

    # Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸
    print("\n" + "=" * 60)
    print("RECOMMENDATIONS")
    print("=" * 60)

    if report.get('quality_percentage', 0) >= 80:
        print("âœ… Corpus quality is EXCELLENT for RAG testing!")
        print("   - Documents are unique and fictional")
        print("   - Good cross-references between documents")
        print("   - No contamination with original terms")
    elif report.get('quality_percentage', 0) >= 60:
        print("âš ï¸  Corpus quality is ACCEPTABLE for RAG testing")
        print("   - Some minor issues detected")
        print("   - Still usable for testing purposes")
    else:
        print("âŒ Corpus quality needs IMPROVEMENT")
        print("   - Significant issues detected")
        print("   - Consider regenerating the corpus")

    print(f"\nğŸ“Š Final assessment: {report.get('quality_percentage', 0):.1f}%")

if __name__ == "__main__":
    main()